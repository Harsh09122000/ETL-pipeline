{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNlw8YvBf5qPwcubajxh2ZC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["SparkSession - hive\n","\n","SparkContext\n","\n","Spark UI\n","\n","Version\n","v3.5.0\n","Master\n","local[8]\n","AppName\n","Databricks Shell"],"metadata":{"id":"JK-9j1SfUPaX"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"Sales Data ETL Pipeline\").getOrCreate()"],"metadata":{"id":"vHvLx2alSDok"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Amazon IAM user credentials for accessing S3 bucket and fetching data."],"metadata":{"id":"Nuxr8ipBSEqC"}},{"cell_type":"code","source":["access_key = '###########################'\n","secret_key = '####################################\n","sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n","sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n","\n","# If you are using Auto Loader file notification mode to load files, provide the AWS Region ID.\n","aws_region = \"ap-south-1\"\n","sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + aws_region + \".amazonaws.com\")"],"metadata":{"id":"ScPgRAc5R8ye"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, IntegerType, FloatType,StringType\n","from pyspark.sql.functions import col"],"metadata":{"id":"6rfkmkY_SL_x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Schema for datasets"],"metadata":{"id":"kis9N4GHSRFw"}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, TimestampType, DateType\n","\n","# Define the schema\n","schema = StructType([\n","    StructField(\"Transaction_ID\", StringType(), True),\n","    StructField(\"Customer_ID\", StringType(), True),\n","    StructField(\"Name\", StringType(), True),\n","    StructField(\"Email\", StringType(), True),\n","    StructField(\"Phone\", StringType(), True),\n","    StructField(\"Address\", StringType(), True),\n","    StructField(\"City\", StringType(), True),\n","    StructField(\"State\", StringType(), True),\n","    StructField(\"Zipcode\", StringType(), True),\n","    StructField(\"Country\", StringType(), True),\n","    StructField(\"Age\", FloatType(), True),\n","    StructField(\"Gender\", StringType(), True),\n","    StructField(\"Income\", StringType(), True),\n","    StructField(\"Customer_Segment\", StringType(), True),\n","    StructField(\"Date\", StringType() , True),  # Date in 'MM/dd/yyyy' format\n","    StructField(\"Year\", StringType() , True),  # Year as integer\n","    StructField(\"Month\", StringType(), True),  # Month as string\n","    StructField(\"Time\", StringType(), True),  # Time in 'HH:mm:ss' format\n","    StructField(\"Total_Purchases\", FloatType(), True),\n","    StructField(\"Amount\", FloatType(), True),\n","    StructField(\"Total_Amount\", FloatType(), True),\n","    StructField(\"Product_Category\", StringType(), True),\n","    StructField(\"Product_Brand\", StringType(), True),\n","    StructField(\"Product_Type\", StringType(), True),\n","    StructField(\"Feedback\", StringType(), True),\n","    StructField(\"Shipping_Method\", StringType(), True),\n","    StructField(\"Payment_Method\", StringType(), True),\n","    StructField(\"Order_Status\", StringType(), True),\n","    StructField(\"Ratings\", FloatType(), True),\n","    StructField(\"products\", StringType(), True)\n","])\n"],"metadata":{"id":"CzDl-Y7kSPai"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading Datasets"],"metadata":{"id":"QoMe0afdSric"}},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(\"s3://osamaharsh/new_retail_data.csv\")"],"metadata":{"id":"Gv6dXBmrSoPb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.show(1)"],"metadata":{"id":"7Py_h4Q0Su_0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import to_date, col\n","\n","# Convert the Date column from string to DateType\n","df = df.withColumn(\"Date\", to_date(col(\"Date\"), \"M/d/yyyy\"))\n","\n","# Convert the Year column to IntegerType\n","df = df.withColumn(\"Year\", col(\"Year\").cast(IntegerType()))\n","\n","\n","\n","# Show the DataFrame to verify the schema and data\n","df.show(5)"],"metadata":{"id":"9ikQomZzS1xH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"id":"eh0sXaOqS77v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_cols = len(df.columns)\n","print(f\"Number of columns in the DataFrame: {num_cols}\")"],"metadata":{"id":"l30iK20UTA9U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#Analyzing Data"],"metadata":{"id":"TqLlJ9ulTFQD"}},{"cell_type":"code","source":["df.count()"],"metadata":{"id":"disNeymlTGIU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Drop all rows with any NULL values\n","df = df.dropna()\n","df.show()\n","df.count()"],"metadata":{"id":"0nF16WGvTJKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df= df.dropDuplicates()"],"metadata":{"id":"8bqzN6sUTL8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col, sum\n","\n","# Initialize a list to hold the results\n","null_counts = {}\n","\n","# Iterate over each column in the DataFrame\n","for column in df.columns:\n","    null_count = df.filter(col(column).isNull()).count()\n","    null_counts[column] = null_count\n","\n","# Print the count of NULL values for each column\n","for column, count in null_counts.items():\n","    print(f\"Column '{column}' has {count} NULL values.\")"],"metadata":{"id":"D--nyaJ_TOP5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Get the count of unique values for each column\n","unique_counts = {col_name: df.select(col_name).distinct().count() for col_name in df.columns}\n","\n","# Display the unique counts for each column\n","for col_name, count in unique_counts.items():\n","    print(f\"{col_name}: {count} unique values\")"],"metadata":{"id":"jaunAMMgTS_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col, count\n","\n","# Step 1: Group by Transaction_ID and Customer_ID, and count occurrences\n","duplicates_df = df.groupBy(\"Transaction_ID\", \"Customer_ID\").agg(count(\"*\").alias(\"count\"))\n","\n","# Step 2: Filter for Transaction_IDs with duplicates\n","duplicate_transactions = duplicates_df.filter(col(\"count\") > 1)\n","\n","# Step 3: Count how many duplicate Transaction_IDs have the same Customer_ID\n","matching_cust_ids_count = duplicate_transactions.count()\n","\n","print(f\"Number of duplicate Transaction_IDs with the same Customer_ID: {matching_cust_ids_count}\")"],"metadata":{"id":"sORbrPspTT-i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import count\n","\n","duplicate_count = df.groupBy(df.columns).agg(count(\"*\").alias(\"count\")).filter(col(\"count\") > 1).count()\n","print(f\"Number of duplicate rows in the DataFrame: {duplicate_count}\")"],"metadata":{"id":"TD1rRPh8TXQK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Count of Transactions Per Year"],"metadata":{"id":"uZxyqEvTTcNn"}},{"cell_type":"code","source":["df.createOrReplaceTempView(\"sales_data\")"],"metadata":{"id":"vcJAICloTaeI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%sql\n","SELECT Year, COUNT(Transaction_ID) AS transaction_count\n","FROM sales_data\n","GROUP BY Year\n","ORDER BY Year;"],"metadata":{"id":"gzEgxICwTjUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Total Amount Spent by top 10 customer"],"metadata":{"id":"OdmC4pG4TmoB"}},{"cell_type":"code","source":["%sql\n","SELECT Customer_ID, SUM(Total_Amount) AS total_spent\n","FROM sales_data\n","GROUP BY Customer_ID\n","ORDER BY total_spent DESC limit 10;\n"],"metadata":{"id":"Gw_NkzJRTlJ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Average Age of Customers by Customer Segment"],"metadata":{"id":"DEY4I_WyTqvl"}},{"cell_type":"code","source":["%sql\n","SELECT Customer_Segment, AVG(Age) AS avg_age\n","FROM sales_data\n","GROUP BY Customer_Segment\n","ORDER BY avg_age DESC;"],"metadata":{"id":"w8IhIGqJTtbO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Top 10 Products by Total Sales Amount"],"metadata":{"id":"bUUIar1hTt-B"}},{"cell_type":"code","source":["%sql\n","SELECT Product_Brand, Product_Type, SUM(Total_Amount) AS total_sales\n","FROM sales_data\n","GROUP BY Product_Brand, Product_Type\n","ORDER BY total_sales DESC\n","LIMIT 10;"],"metadata":{"id":"lAVcgthaTw7T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Monthly Sales Performance"],"metadata":{"id":"i0HNk2H8TyrO"}},{"cell_type":"code","source":["%sql\n","SELECT Year, Month, SUM(Total_Amount) AS monthly_sales\n","FROM sales_data\n","GROUP BY Year, Month\n","ORDER BY Year, Month;"],"metadata":{"id":"ATLvu95eT0y0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Distribution of Order Status"],"metadata":{"id":"ku8oRWAWT3v-"}},{"cell_type":"code","source":["%sql\n","SELECT Order_Status, COUNT(*) AS order_count\n","FROM sales_data\n","GROUP BY Order_Status\n","ORDER BY order_count DESC;"],"metadata":{"id":"8NSpjAX9T4TL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Busiest Hours"],"metadata":{"id":"fpVjLq87T540"}},{"cell_type":"code","source":["%sql\n","-- SQL query to find the busiest hours overall\n","SELECT\n","    EXTRACT(HOUR FROM CAST(Time AS TIMESTAMP)) AS Hour,\n","    COUNT(*) AS order_count\n","FROM sales_data\n","GROUP BY Hour\n","ORDER BY order_count DESC;"],"metadata":{"id":"PZbmY-bMT70C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Most Orders Country-Wise"],"metadata":{"id":"m03BfS0RT-BU"}},{"cell_type":"code","source":["%sql\n","-- SQL query to find the most orders by country\n","SELECT\n","    Country,\n","    COUNT(*) AS order_count\n","FROM sales_data\n","GROUP BY Country\n","ORDER BY order_count DESC;"],"metadata":{"id":"eo7L-T1eUAYj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Most order regionwise"],"metadata":{"id":"iQ1KTUsgUB93"}},{"cell_type":"code","source":["%sql\n","-- SQL query to find the most orders by region\n","SELECT\n","    state,\n","    COUNT(*) AS order_count\n","FROM sales_data\n","GROUP BY state\n","ORDER BY order_count DESC;\n"],"metadata":{"id":"9xkK_XGmUFMo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","#Loading Data in S3 bucket in parquet format"],"metadata":{"id":"q1mYKoCaUHc1"}},{"cell_type":"code","source":["result.coalesce(1).write.parquet('s3a://trypiyush/try123.parquet')"],"metadata":{"id":"7c24V0eCULip"},"execution_count":null,"outputs":[]}]}